\begin{abstract}
Large Language Models (LLMs) are predominantly trained on English corpora, raising the hypothesis that they process non-English inputs via an ``implicit internal translation'' mechanism. This study investigates this hypothesis by evaluating GPT-4o across 12 languages varying in resource availability (High, Mid, Low) using the MuBench dataset. We employ novel metrics---Agreement with English and Same Mistake Ratio---to quantify the extent to which non-English reasoning aligns with English reasoning on identical tasks. We find that for high and mid-resource languages (e.g., French, Vietnamese), the model exhibits high response agreement (76--78\%) and a significant ``Same Mistake Ratio'' (up to 0.64), suggesting that errors are shared and likely stem from a common, English-centric latent representation. Conversely, for low-resource languages (e.g., Tamil), performance degrades significantly (52\% accuracy), and error patterns become uncorrelated with English (Same Mistake Ratio $\approx$ 0.33, equivalent to random chance). These results suggest that while ``implicit translation'' may effectively support mid-resource languages, this mechanism breaks down for under-represented languages, leading to distinct and inferior failure modes.
\end{abstract}
